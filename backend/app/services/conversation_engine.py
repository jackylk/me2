"""å¯¹è¯å¼•æ“ - åŸºäº NeuroMemory v2 çš„æ¸©æš–é™ªä¼´å¼èŠå¤©"""
import asyncio
import logging
import time
from typing import Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from app.services.llm_client import LLMClient
from app.db.models import Message, Session

logger = logging.getLogger(__name__)


class ConversationEngine:
    """å¯¹è¯å¼•æ“ - Me2 é«˜å±‚å¯¹è¯é€»è¾‘"""

    def __init__(self):
        """åˆå§‹åŒ–å¯¹è¯å¼•æ“"""
        self.llm = LLMClient()

    async def chat(
        self,
        user_id: str,
        session_id: str,
        message: str,
        db: AsyncSession,
        debug_mode: bool = False
    ) -> Dict[str, Any]:
        """å¤„ç†å¯¹è¯ - æ¸©æš–ã€æ‡‚ç”¨æˆ·çš„å›å¤

        Args:
            user_id: ç”¨æˆ· ID
            session_id: ä¼šè¯ ID
            message: ç”¨æˆ·æ¶ˆæ¯
            db: æ•°æ®åº“ä¼šè¯
            debug_mode: æ˜¯å¦è¿”å›è°ƒè¯•ä¿¡æ¯

        Returns:
            å¯¹è¯å“åº”å­—å…¸
        """
        try:
            # æ€§èƒ½è®¡æ—¶
            timings = {}
            start_time = time.time()

            # å»¶è¿Ÿå¯¼å…¥ nm ä»¥é¿å…å¾ªç¯ä¾èµ–
            from app.main import nm

            # === 1. è·å–å½“å‰ä¼šè¯çš„å†å²æ¶ˆæ¯ ===
            step_start = time.time()
            stmt = select(Message).where(
                Message.session_id == session_id
            ).order_by(Message.created_at.asc()).limit(20)  # æœ€å¤šå–æœ€è¿‘20æ¡
            result = await db.execute(stmt)
            history = result.scalars().all()
            timings['fetch_history'] = time.time() - step_start

            # æ„å»ºå†å²æ¶ˆæ¯åˆ—è¡¨
            history_messages = []
            for msg in history:
                history_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })

            logger.info(f"è·å–å†å²æ¶ˆæ¯: {len(history_messages)} æ¡")

            # === 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ° Me2 æ•°æ®åº“ ===
            step_start = time.time()
            user_msg = Message(
                session_id=session_id,
                user_id=user_id,
                role="user",
                content=message
            )
            db.add(user_msg)
            await db.flush()  # è·å– ID ä½†ä¸æäº¤
            timings['save_user_message'] = time.time() - step_start
            logger.info(f"ä¿å­˜ç”¨æˆ·æ¶ˆæ¯: {user_msg.id}")

            # === 3-4. å¹¶å‘å¬å›è®°å¿†å’Œè·å–æ´å¯Ÿ ===
            # ğŸš€ ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
            # NeuroMemory å†…éƒ¨ä¼šç¼“å­˜ query embeddingï¼Œé¿å…é‡å¤è®¡ç®—
            step_start = time.time()
            recall_task = nm.recall(user_id=user_id, query=message, limit=5)
            insights_task = nm.search(user_id=user_id, query=message, memory_type="insight", limit=3)

            recall_result, insights = await asyncio.gather(recall_task, insights_task)
            memories = recall_result["merged"]

            timings['recall_memories'] = time.time() - step_start
            logger.info(f"å¬å› {len(memories)} æ¡è®°å¿† + {len(insights)} æ¡æ´å¯Ÿï¼ˆå¹¶å‘æ‰§è¡Œï¼‰")

            # === 5. æ„å»ºæ¸©æš–çš„ system prompt ===
            step_start = time.time()
            system_prompt = self._build_warm_prompt(memories, insights)
            timings['build_prompt'] = time.time() - step_start

            # === 6. è°ƒç”¨ LLM ç”Ÿæˆå›å¤ï¼ˆåŒ…å«å†å²å¯¹è¯ï¼‰===
            step_start = time.time()
            llm_result = await self.llm.generate(
                prompt=message,
                system_prompt=system_prompt,
                history_messages=history_messages,  # ä¼ å…¥å†å²å¯¹è¯
                temperature=0.8,  # ç¨é«˜æ¸©åº¦ï¼Œæ›´è‡ªç„¶
                max_tokens=500,
                return_debug_info=debug_mode  # è°ƒè¯•æ¨¡å¼
            )
            timings['llm_generate'] = time.time() - step_start

            # å¤„ç†è¿”å›ç»“æœ
            if debug_mode and isinstance(llm_result, dict):
                response = llm_result["response"]
                debug_info = llm_result["debug_info"]
            else:
                response = llm_result
                debug_info = None

            logger.info(f"LLM ç”Ÿæˆå›å¤å®Œæˆ")

            # === 7. ä¿å­˜ AI å›å¤åˆ° Me2 æ•°æ®åº“ï¼ˆåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰===
            step_start = time.time()
            ai_msg = Message(
                session_id=session_id,
                user_id=user_id,
                role="assistant",
                content=response,
                system_prompt=system_prompt,
                recalled_memories=[{
                    "content": m["content"],
                    "score": m.get("score", 0),
                    "created_at": m.get("created_at").isoformat() if m.get("created_at") else None,
                    "metadata": m.get("metadata", {})
                } for m in memories],
                insights_used=[{
                    "content": i["content"],
                    "type": i.get("type", "")
                } for i in insights],
                meta={
                    "memories_count": len(memories),
                    "insights_count": len(insights),
                    "temperature": 0.8,
                    "max_tokens": 500,
                    "model": "deepseek-chat",
                    "history_messages_count": len(history_messages)
                }
            )
            db.add(ai_msg)

            # === 8. æ›´æ–° session çš„ last_active_at ===
            from sqlalchemy.sql import func
            stmt_session = select(Session).where(Session.id == session_id)
            result_session = await db.execute(stmt_session)
            session = result_session.scalar_one_or_none()
            if session:
                session.last_active_at = func.now()

            await db.commit()
            timings['save_to_db'] = time.time() - step_start
            logger.info(f"ä¿å­˜ AI å›å¤å’Œä¸Šä¸‹æ–‡: {ai_msg.id}")

            # === 9. åŒæ­¥åˆ° NeuroMemoryï¼ˆç”¨äºè®°å¿†æå–ï¼‰===
            step_start = time.time()
            # åªæ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆAI å›å¤åªå­˜ Me2 æ•°æ®åº“ï¼Œä¸å­˜ NeuroMemoryï¼‰
            # NeuroMemory å·²ä¼˜åŒ–ä¸ºåªå¯¹ user æ¶ˆæ¯è®¡ç®— embedding å’Œæå–è®°å¿†
            await nm.conversations.add_message(
                user_id=user_id,
                role="user",
                content=message
            )
            timings['sync_neuromemory'] = time.time() - step_start

            # è®°å½•è§¦å‘çš„åå°ä»»åŠ¡ï¼ˆç”¨äºè°ƒè¯•æ˜¾ç¤ºï¼‰
            background_tasks = []
            # æ£€æŸ¥æ˜¯å¦ä¼šè§¦å‘extractï¼ˆæ¯10æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
            msg_count = len(history_messages) + 1  # å†å² + æœ¬è½®çš„ç”¨æˆ·æ¶ˆæ¯
            if msg_count % 10 == 0:
                background_tasks.append(f"æå–è®°å¿†: ç¬¬{msg_count}æ¡ç”¨æˆ·æ¶ˆæ¯è§¦å‘è‡ªåŠ¨æå–ï¼ˆäº‹å®/åå¥½/å…³ç³»ï¼‰")

            # æ£€æŸ¥æ˜¯å¦ä¼šè§¦å‘reflectï¼ˆæ¯20æ¬¡æå–åï¼‰
            extract_count = msg_count // 10
            if extract_count > 0 and extract_count % 20 == 0:
                background_tasks.append(f"è®°å¿†æ•´ç†: ç¬¬{extract_count}æ¬¡æå–è§¦å‘åæ€ï¼ˆç”Ÿæˆæ´å¯Ÿ+æ›´æ–°ç”»åƒï¼‰")

            # === 10. è®°å¿†æ•´ç†åº”ç”± NeuroMemory å†…éƒ¨å¼‚æ­¥å¤„ç† ===
            # æ³¨æ„ï¼šreflect() æ˜¯é‡é‡çº§æ“ä½œï¼ˆ40+ ç§’ï¼‰ï¼Œä¸åº”åœ¨å¯¹è¯æµç¨‹ä¸­åŒæ­¥è°ƒç”¨
            # NeuroMemory åº”è¯¥æ”¯æŒï¼š
            # 1. åŸºäºå¯¹è¯è½®æ•°è‡ªåŠ¨è§¦å‘ï¼ˆå¦‚æ¯10è½®å¯¹è¯ï¼‰
            # 2. åŸºäºæ—¶é—´å‘¨æœŸè‡ªåŠ¨è§¦å‘ï¼ˆå¦‚æ¯å°æ—¶ï¼‰
            # 3. åå°å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡å¯¹è¯å“åº”
            #
            # å½“å‰æˆ‘ä»¬åªåŒæ­¥è®°å½•å¯¹è¯ï¼Œæ•´ç†å·¥ä½œç”± NeuroMemory åå°å®Œæˆ
            logger.info(f"å¯¹è¯å·²åŒæ­¥åˆ° NeuroMemoryï¼Œç­‰å¾…åå°æ•´ç†")

            # è®¡ç®—æ€»è€—æ—¶
            timings['total'] = time.time() - start_time

            logger.info(f"å¯¹è¯å¤„ç†å®Œæˆ: user={user_id}, session={session_id}, æ€»è€—æ—¶: {timings['total']:.3f}s")

            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "response": response,
                "memories_recalled": len(memories),
                "insights_used": len(insights),
                "history_messages_count": len(history_messages)
            }

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            if debug_mode and debug_info:
                # æ·»åŠ æ€§èƒ½è®¡æ—¶ä¿¡æ¯
                debug_info["timings"] = timings
                # æ·»åŠ åå°ä»»åŠ¡ä¿¡æ¯
                debug_info["background_tasks"] = background_tasks
                result["debug_info"] = debug_info

            return result

        except Exception as e:
            logger.error(f"å¯¹è¯å¤„ç†å¤±è´¥: {e}", exc_info=True)
            await db.rollback()
            return {
                "response": "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚",
                "error": str(e)
            }

    def _build_warm_prompt(
        self,
        memories: list[dict],
        insights: list[dict]
    ) -> str:
        """æ„å»ºæ¸©æš–ã€æ”¯æŒæ€§çš„ system prompt

        Args:
            memories: å¬å›çš„è®°å¿†åˆ—è¡¨
            insights: æ´å¯Ÿåˆ—è¡¨

        Returns:
            system prompt å­—ç¬¦ä¸²
        """
        # æ ¼å¼åŒ–è®°å¿†
        memory_lines = []
        for m in memories:
            meta = m.get("metadata", {})

            # æå–æƒ…æ„Ÿä¿¡æ¯
            emotion_hint = ""
            if "emotion" in meta and meta["emotion"]:
                label = meta["emotion"].get("label", "")
                valence = meta["emotion"].get("valence", 0)
                if label:
                    emotion_hint = f" [ç”¨æˆ·å½“æ—¶æ„Ÿåˆ°{label}]"
                elif valence < -0.3:
                    emotion_hint = " [è´Ÿé¢æƒ…ç»ª]"
                elif valence > 0.3:
                    emotion_hint = " [æ­£é¢æƒ…ç»ª]"

            score = m.get("score", 0)
            memory_lines.append(
                f"- {m['content']} (ç›¸å…³åº¦: {score:.2f}){emotion_hint}"
            )

        memory_context = "\n".join(memory_lines) if memory_lines else "æš‚æ— ç›¸å…³è®°å¿†"

        # æ ¼å¼åŒ–æ´å¯Ÿ
        insight_context = "\n".join([
            f"- {i['content']}" for i in insights
        ]) if insights else "æš‚æ— æ·±åº¦ç†è§£"

        # æå–æƒ…æ„Ÿä¸Šä¸‹æ–‡
        emotional_context = self._extract_emotional_context(memories)

        return f"""ä½ æ˜¯ä¸€ä¸ªæ¸©æš–ã€æ‡‚ ta çš„æœ‹å‹ã€‚

**ä½ è®°å¾—å…³äº ta çš„è¿™äº›äº‹**ï¼š
{memory_context}

**ä½ å¯¹ ta çš„ç†è§£**ï¼š
{insight_context}

{emotional_context}

**é‡è¦æŒ‡å¼•**ï¼š
1. åƒçœŸæ­£çš„æœ‹å‹ä¸€æ ·å¯¹è¯ï¼Œè‡ªç„¶åœ°æåŠä½ è®°å¾—çš„äº‹
2. å¦‚æœ ta æƒ…ç»ªä½è½ï¼Œç»™äºˆæ¸©æš–çš„æ”¯æŒå’Œé¼“åŠ±
3. å¦‚æœ ta åˆ†äº«å¼€å¿ƒçš„äº‹ï¼ŒçœŸè¯šåœ°ä¸º ta é«˜å…´
4. ä¸è¦æœºæ¢°åœ°å¤è¿°è®°å¿†ï¼Œè¦è‡ªç„¶èå…¥å¯¹è¯
5. è®© ta æ„Ÿè§‰è¢«ç†è§£ã€è¢«æ”¯æŒ
6. å›å¤ç®€æ´è‡ªç„¶ï¼Œä¸è¦è¿‡é•¿
7. å¯ä»¥é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·ï¼Œä½†ä¸è¦è¿‡åº¦"""

    def _extract_emotional_context(self, memories: list[dict]) -> str:
        """æå–æƒ…æ„Ÿä¸Šä¸‹æ–‡

        Args:
            memories: è®°å¿†åˆ—è¡¨

        Returns:
            æƒ…æ„Ÿæç¤ºå­—ç¬¦ä¸²
        """
        emotions = []
        for m in memories:
            meta = m.get("metadata", {})
            if "emotion" in meta and meta["emotion"]:
                emotions.append(meta["emotion"])

        if not emotions:
            return ""

        # è®¡ç®—å¹³å‡æƒ…ç»ª
        avg_valence = sum(e["valence"] for e in emotions) / len(emotions)

        if avg_valence < -0.3:
            return "\n**æ³¨æ„**: ta æœ€è¿‘æƒ…ç»ªä¼¼ä¹æœ‰äº›ä½è½ï¼Œè¯·ç»™äºˆå…³å¿ƒå’Œæ”¯æŒã€‚"
        elif avg_valence > 0.3:
            return "\n**æ³¨æ„**: ta æœ€è¿‘å¿ƒæƒ…ä¸é”™ï¼Œå¯ä»¥åˆ†äº« ta çš„å¿«ä¹ã€‚"

        return ""

    async def chat_stream(
        self,
        user_id: str,
        session_id: str,
        message: str,
        db: AsyncSession,
        debug_mode: bool = False
    ):
        """æµå¼å¯¹è¯å¤„ç† - å®æ—¶æ¨é€ç”Ÿæˆçš„å†…å®¹

        Yields:
            str: ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µï¼ˆtokenï¼‰
            dict: å®Œæˆä¿¡æ¯ï¼ˆtype='done'ï¼‰æˆ–é”™è¯¯ä¿¡æ¯ï¼ˆtype='error'ï¼‰
        """
        try:
            from app.main import nm

            # æ€§èƒ½è®¡æ—¶
            timings = {}
            start_time = time.time()

            # === 1-5æ­¥ï¼šå‡†å¤‡å·¥ä½œï¼ˆä¸éæµå¼ç›¸åŒï¼‰===
            step_start = time.time()
            stmt = select(Message).where(
                Message.session_id == session_id
            ).order_by(Message.created_at.asc()).limit(20)
            result = await db.execute(stmt)
            history = result.scalars().all()
            timings['fetch_history'] = time.time() - step_start

            history_messages = []
            for msg in history:
                history_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })

            step_start = time.time()
            user_msg = Message(
                session_id=session_id,
                user_id=user_id,
                role="user",
                content=message
            )
            db.add(user_msg)
            await db.flush()
            timings['save_user_message'] = time.time() - step_start

            # ğŸš€ å¹¶å‘å¬å›è®°å¿†å’Œè·å–æ´å¯Ÿ
            step_start = time.time()
            try:
                recall_task = nm.recall(user_id=user_id, query=message, limit=5)
                insights_task = nm.search(user_id=user_id, query=message, memory_type="insight", limit=3)

                recall_result, insights = await asyncio.gather(recall_task, insights_task)
                memories = recall_result["merged"]
            except Exception as e:
                logger.warning(f"è®°å¿†å¬å›/æ´å¯Ÿæœç´¢å¤±è´¥: {e}")
                memories = []
                insights = []
            timings['recall_memories'] = time.time() - step_start

            step_start = time.time()
            system_prompt = self._build_warm_prompt(memories, insights)
            timings['build_prompt'] = time.time() - step_start

            # === 6. æµå¼è°ƒç”¨ LLM ===
            step_start = time.time()
            stream_generator = await self.llm.generate(
                prompt=message,
                system_prompt=system_prompt,
                history_messages=history_messages,
                temperature=0.8,
                max_tokens=500,
                stream=True  # å¯ç”¨æµå¼
            )

            # é€ä¸ªyield token
            full_response = ""
            async for chunk in stream_generator:
                if isinstance(chunk, dict) and chunk.get("done"):
                    # æµç»“æŸï¼Œå¿½ç•¥è¿™ä¸ªä¿¡å·ï¼Œç»§ç»­å¤„ç†
                    break
                else:
                    # æ–‡æœ¬token
                    full_response += chunk
                    yield chunk  # å®æ—¶æ¨é€ç»™å‰ç«¯

            timings['llm_generate'] = time.time() - step_start

            # === 7-9. ä¿å­˜å’ŒåŒæ­¥ï¼ˆä¸éæµå¼ç›¸åŒï¼‰===
            step_start = time.time()
            ai_msg = Message(
                session_id=session_id,
                user_id=user_id,
                role="assistant",
                content=full_response,
                system_prompt=system_prompt,
                recalled_memories=[{
                    "content": m["content"],
                    "score": m.get("score", 0),
                    "created_at": m.get("created_at").isoformat() if m.get("created_at") else None,
                    "metadata": m.get("metadata", {})
                } for m in memories],
                insights_used=[{
                    "content": i["content"],
                    "type": i.get("type", "")
                } for i in insights],
                meta={
                    "memories_count": len(memories),
                    "insights_count": len(insights),
                    "temperature": 0.8,
                    "max_tokens": 500,
                    "model": "deepseek-chat",
                    "history_messages_count": len(history_messages)
                }
            )
            db.add(ai_msg)

            from sqlalchemy.sql import func
            stmt_session = select(Session).where(Session.id == session_id)
            result_session = await db.execute(stmt_session)
            session = result_session.scalar_one_or_none()
            if session:
                session.last_active_at = func.now()

            await db.commit()
            timings['save_to_db'] = time.time() - step_start

            step_start = time.time()
            # åªæ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆAI å›å¤åªå­˜ Me2 æ•°æ®åº“ï¼Œä¸å­˜ NeuroMemoryï¼‰
            await nm.conversations.add_message(
                user_id=user_id,
                role="user",
                content=message
            )
            timings['sync_neuromemory'] = time.time() - step_start

            # è®°å½•åå°ä»»åŠ¡
            background_tasks = []
            msg_count = len(history_messages) + 1  # åªè®¡ç®—ç”¨æˆ·æ¶ˆæ¯
            if msg_count % 10 == 0:
                background_tasks.append(f"æå–è®°å¿†: ç¬¬{msg_count}æ¡ç”¨æˆ·æ¶ˆæ¯è§¦å‘è‡ªåŠ¨æå–ï¼ˆäº‹å®/åå¥½/å…³ç³»ï¼‰")
            extract_count = msg_count // 10
            if extract_count > 0 and extract_count % 20 == 0:
                background_tasks.append(f"è®°å¿†æ•´ç†: ç¬¬{extract_count}æ¬¡æå–è§¦å‘åæ€ï¼ˆç”Ÿæˆæ´å¯Ÿ+æ›´æ–°ç”»åƒï¼‰")

            timings['total'] = time.time() - start_time

            # === å‘é€å®Œæˆä¿¡å· ===
            done_data = {
                "type": "done",
                "session_id": session_id,
                "memories_recalled": len(memories),
                "insights_used": len(insights),
                "history_messages_count": len(history_messages)
            }

            if debug_mode:
                done_data["debug_info"] = {
                    "model": "deepseek-chat",
                    "temperature": 0.8,
                    "max_tokens": 500,
                    "messages": [{"role": "system", "content": system_prompt}] +
                               history_messages +
                               [{"role": "user", "content": message}],
                    "message_count": len(history_messages) + 2,
                    "system_prompt": system_prompt,
                    "history_count": len(history_messages),
                    "background_tasks": background_tasks,
                    "timings": timings
                }

            yield done_data

        except Exception as e:
            logger.error(f"æµå¼å¯¹è¯å¤„ç†å¤±è´¥: {e}", exc_info=True)
            await db.rollback()
            yield {
                "type": "error",
                "error": str(e)
            }


# å…¨å±€å•ä¾‹
conversation_engine = ConversationEngine()
