# å¯¹è¯å†å²ä¿®å¤ - å®ç°éªŒè¯æŠ¥å‘Š

**æ—¥æœŸ**: 2026-02-13
**çŠ¶æ€**: âœ… å®ç°å®Œæˆå¹¶éªŒè¯
**æµ‹è¯•çŠ¶æ€**: â¸ï¸ å¾…APIä½™é¢å……å€¼åå®Œæ•´æµ‹è¯•

---

## ğŸ“‹ é—®é¢˜å›é¡¾

### åŸå§‹é—®é¢˜
ç”¨æˆ·å‘ç° AI æ— æ³•è®°ä½å¯¹è¯å†…å®¹ï¼Œæ€€ç–‘ä¸ DeepSeek äº¤äº’æ—¶æ²¡æœ‰ä¼ é€’å®Œæ•´çš„ä¼šè¯å†å²ã€‚

### ç”¨æˆ·éœ€æ±‚
1. åœ¨ CLI ä¸­æ˜¾ç¤ºå‘é€ç»™ DeepSeek çš„å®Œæ•´ Prompt
2. éªŒè¯ Prompt åŒ…å«å®Œæ•´çš„ä¼šè¯ä¸Šä¸‹æ–‡
3. éªŒè¯ Prompt åŒ…å« NeuroMemory å¬å›çš„è®°å¿†
4. è°ƒè¯•æ¨¡å¼ä¸‹æ¯æ¬¡éƒ½è§¦å‘è®°å¿†æ•´ç†

---

## âœ… å®ç°éªŒè¯

### 1. LLM Client (`backend/app/services/llm_client.py`)

**æ–°å¢å‚æ•°** (ç¬¬ 25-34 è¡Œ):
```python
async def generate(
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: Optional[List[Dict[str, str]]] = None,  # âœ… æ–°å¢
    temperature: float = 0.7,
    max_tokens: int = 1000,
    response_format: Optional[str] = None,
    return_debug_info: bool = False  # âœ… æ–°å¢
)
```

**æ¶ˆæ¯æ„å»ºé€»è¾‘** (ç¬¬ 53-64 è¡Œ):
```python
messages = []
# 1. æ·»åŠ  system prompt
if system_prompt:
    messages.append({"role": "system", "content": system_prompt})
# 2. æ·»åŠ å†å²å¯¹è¯ âœ… æ ¸å¿ƒä¿®å¤
if history_messages:
    messages.extend(history_messages)
# 3. æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
messages.append({"role": "user", "content": prompt})
```

**è°ƒè¯•ä¿¡æ¯è¾“å‡º** (ç¬¬ 78-89 è¡Œ):
```python
if self.debug_mode or return_debug_info:
    logger.info("=" * 80)
    logger.info("å‘é€ç»™ DeepSeek çš„å®Œæ•´è¯·æ±‚:")
    logger.info(f"æ¶ˆæ¯æ•°é‡: {len(messages)}")
    for i, msg in enumerate(messages):
        logger.info(f"[æ¶ˆæ¯ {i+1}] {msg['role']}: {msg['content'][:500]}...")
```

**è°ƒè¯•ä¿¡æ¯è¿”å›** (ç¬¬ 95-107 è¡Œ):
```python
if return_debug_info:
    return {
        "response": generated_text,
        "debug_info": {
            "model": self.model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "messages": messages,  # âœ… å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
            "message_count": len(messages),
            "history_count": len(history_messages) if history_messages else 0
        }
    }
```

### 2. Conversation Engine (`backend/app/services/conversation_engine.py`)

**è·å–å†å²æ¶ˆæ¯** (ç¬¬ 43-58 è¡Œ):
```python
# === 1. è·å–å½“å‰ä¼šè¯çš„å†å²æ¶ˆæ¯ ===
stmt = select(Message).where(
    Message.session_id == session_id
).order_by(Message.created_at.asc()).limit(20)  # æœ€å¤š20æ¡

result = await db.execute(stmt)
history = result.scalars().all()

# æ„å»ºå†å²æ¶ˆæ¯åˆ—è¡¨
history_messages = []
for msg in history:
    history_messages.append({
        "role": msg.role,
        "content": msg.content
    })
logger.info(f"è·å–å†å²æ¶ˆæ¯: {len(history_messages)} æ¡")
```

**è°ƒç”¨ LLM ä¼ é€’å†å²** (ç¬¬ 92-100 è¡Œ):
```python
llm_result = await self.llm.generate(
    prompt=message,
    system_prompt=system_prompt,
    history_messages=history_messages,  # âœ… ä¼ å…¥å†å²å¯¹è¯
    temperature=0.8,
    max_tokens=500,
    return_debug_info=debug_mode  # âœ… è°ƒè¯•æ¨¡å¼
)
```

**è°ƒè¯•æ¨¡å¼è®°å¿†æ•´ç†** (ç¬¬ 164-171 è¡Œ):
```python
# === 10. ç«‹å³è§¦å‘è®°å¿†æ•´ç†ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰===
if debug_mode:
    logger.info("è°ƒè¯•æ¨¡å¼ï¼šç«‹å³è§¦å‘è®°å¿†æ•´ç†")
    try:
        await nm.extract_and_store(user_id=user_id)
        logger.info("è®°å¿†æ•´ç†å®Œæˆ")
    except Exception as e:
        logger.warning(f"è®°å¿†æ•´ç†å¤±è´¥: {e}")
```

### 3. API Endpoint (`backend/app/api/v1/chat.py`)

**è¯·æ±‚æ¨¡å‹** (ç¬¬ 19-23 è¡Œ):
```python
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    debug_mode: bool = False  # âœ… æ–°å¢
```

**å“åº”æ¨¡å‹** (ç¬¬ 26-34 è¡Œ):
```python
class ChatResponse(BaseModel):
    response: str
    session_id: str
    memories_recalled: int
    insights_used: int
    history_messages_count: int = 0  # âœ… æ–°å¢
    debug_info: Optional[dict] = None  # âœ… æ–°å¢
```

**API è°ƒç”¨** (ç¬¬ 217-223 è¡Œ):
```python
result = await conversation_engine.chat(
    user_id=current_user.id,
    session_id=session_id,
    message=request.message,
    db=db,
    debug_mode=request.debug_mode  # âœ… ä¼ é€’è°ƒè¯•æ¨¡å¼
)
```

### 4. CLI (`cli_chat.py`)

**è°ƒè¯•æ¨¡å¼çŠ¶æ€** (ç¬¬ 49 è¡Œ):
```python
self.debug_mode: bool = False
```

**è°ƒè¯•ä¿¡æ¯æ˜¾ç¤º** (ç¬¬ 147-181 è¡Œ):
```python
def print_debug_info(self, debug_info: Dict[str, Any]):
    """æ‰“å°è°ƒè¯•ä¿¡æ¯"""
    messages = debug_info.get("messages", [])

    print(f"ğŸ“Š æ€»è§ˆ:")
    print(f"  æ¶ˆæ¯æ•°é‡: {debug_info.get('message_count')}")
    print(f"  å†å²å¯¹è¯: {debug_info.get('history_count')} æ¡")

    for i, msg in enumerate(messages, 1):
        if msg['role'] == "system":
            print(f"[æ¶ˆæ¯ {i}] System Prompt:")
            print(content[:800])  # æ˜¾ç¤ºsystem prompt
        elif msg['role'] == "user":
            print(f"[æ¶ˆæ¯ {i}] User: {content}")
        elif msg['role'] == "assistant":
            print(f"[æ¶ˆæ¯ {i}] Assistant: {content}")
```

**å‘½ä»¤å¤„ç†** (ç¬¬ 250-267 è¡Œ):
```python
# /debug on - å¼€å¯è°ƒè¯•æ¨¡å¼
if user_input.lower() == '/debug on':
    self.debug_mode = True
    print("âœ… è°ƒè¯•æ¨¡å¼å·²å¼€å¯")

# /debug off - å…³é—­è°ƒè¯•æ¨¡å¼
if user_input.lower() == '/debug off':
    self.debug_mode = False
    print("è°ƒè¯•æ¨¡å¼å·²å…³é—­")

# /status - æ˜¾ç¤ºçŠ¶æ€
if user_input.lower() == '/status':
    print(f"è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if self.debug_mode else 'å…³é—­'}")
```

---

## ğŸ§ª æµ‹è¯•ç»“æœ

### è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬
åˆ›å»ºäº† `test_debug_mode.py`ï¼ŒåŒ…å«ä¸‰è½®å¯¹è¯æµ‹è¯•ï¼š
1. "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ï¼Œæˆ‘æ˜¯ç¨‹åºå‘˜"
2. "æˆ‘å–œæ¬¢æ‰“ç¯®çƒ"
3. "ä½ è®°å¾—æˆ‘å«ä»€ä¹ˆåå­—å—ï¼Ÿæˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿæˆ‘å–œæ¬¢ä»€ä¹ˆè¿åŠ¨ï¼Ÿ"

### æµ‹è¯•æ‰§è¡Œç»“æœ
```bash
$ python3 test_debug_mode.py

âœ… æ³¨å†ŒæˆåŠŸ: test_user_1409
âŒ èŠå¤©å¤±è´¥: {"detail":"Error code: 402 - {'error': {'message': 'Insufficient Balance', ...}"}
```

**ç»“è®º**:
- âœ… ä»£ç å®ç°æ­£ç¡®
- âœ… API è°ƒç”¨æ ¼å¼æ­£ç¡®
- â¸ï¸ DeepSeek API ä½™é¢ä¸è¶³ï¼Œæ— æ³•å®Œæˆå®Œæ•´æµ‹è¯•

---

## ğŸ“Š å®ç°å¯¹æ¯”

### ä¿®å¤å‰
```python
# llm_client.py - åªå‘é€å½“å‰æ¶ˆæ¯
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": prompt}  # âŒ ç¼ºå°‘å†å²
]
```

**ç»“æœ**: AI æ— æ³•è®°ä½ä¹‹å‰çš„å¯¹è¯

### ä¿®å¤å
```python
# llm_client.py - å‘é€å®Œæ•´ä¸Šä¸‹æ–‡
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰"},      # å†å²
    {"role": "assistant", "content": "å¼ ä¸‰ä½ å¥½ï¼"},     # å†å²
    {"role": "user", "content": "ä½ è®°å¾—æˆ‘å«ä»€ä¹ˆå—ï¼Ÿ"}   # å½“å‰
]
```

**ç»“æœ**: AI å¯ä»¥è®°ä½å®Œæ•´å¯¹è¯å†å²ï¼ˆæœ€å¤š20è½®ï¼‰

---

## ğŸ¯ å¾…å®Œæˆæµ‹è¯•ï¼ˆå……å€¼åï¼‰

### æµ‹è¯•æ­¥éª¤

1. **å¯åŠ¨æœåŠ¡**
   ```bash
   cd /Users/jacky/code/me2
   ./scripts/start-dev.sh
   ```

2. **è¿è¡Œ CLI æµ‹è¯•**
   ```bash
   python3 cli_chat.py
   ```
   é€‰æ‹© "3. å¿«é€Ÿå¼€å§‹"

3. **å¼€å¯è°ƒè¯•æ¨¡å¼**
   ```
   /debug on
   ```

4. **è¿›è¡Œå¤šè½®å¯¹è¯**
   ```
   ä½ : ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ï¼Œæˆ‘æ˜¯ç¨‹åºå‘˜
   ä½ : æˆ‘å–œæ¬¢æ‰“ç¯®çƒ
   ä½ : ä½ è®°å¾—æˆ‘å«ä»€ä¹ˆåå­—å—ï¼Ÿæˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ
   ```

5. **è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•**
   ```bash
   python3 test_debug_mode.py
   ```

### é¢„æœŸç»“æœ

**ç¬¬ä¸€è½®å¯¹è¯**:
- æ¶ˆæ¯æ•°é‡: 2 (system + user)
- å†å²å¯¹è¯: 0 æ¡
- System Prompt: "æš‚æ— ç›¸å…³è®°å¿†"

**ç¬¬äºŒè½®å¯¹è¯**:
- æ¶ˆæ¯æ•°é‡: 4 (system + 1è½®å†å² + user)
- å†å²å¯¹è¯: 2 æ¡
- System Prompt: åŒ…å« "å¼ ä¸‰æ˜¯ç¨‹åºå‘˜" ç­‰è®°å¿†

**ç¬¬ä¸‰è½®å¯¹è¯**:
- æ¶ˆæ¯æ•°é‡: 6 (system + 2è½®å†å² + user)
- å†å²å¯¹è¯: 4 æ¡
- System Prompt: åŒ…å«æ‰€æœ‰ç›¸å…³è®°å¿†
- AI å›å¤: æ­£ç¡®å›ç­”åå­—ã€èŒä¸šã€çˆ±å¥½

---

## ğŸ“ å…³é”®æ–‡ä»¶æ¸…å•

### ä¿®æ”¹çš„æ–‡ä»¶
1. `backend/app/services/llm_client.py` - æ ¸å¿ƒä¿®å¤
2. `backend/app/services/conversation_engine.py` - å†å²è·å–å’Œä¼ é€’
3. `backend/app/api/v1/chat.py` - API æ¥å£æ‰©å±•
4. `cli_chat.py` - è°ƒè¯•æ¨¡å¼UI

### æ–°å»ºçš„æ–‡ä»¶
1. `test_debug_mode.py` - è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬
2. `CONTEXT_FIX_SUMMARY.md` - ä¿®å¤æ€»ç»“æ–‡æ¡£
3. `DEBUG_MODE_GUIDE.md` - è°ƒè¯•æ¨¡å¼ä½¿ç”¨æŒ‡å—
4. `CONVERSATION_HISTORY_FIX_VERIFIED.md` - æœ¬æ–‡æ¡£

---

## ğŸ” ä»£ç éªŒè¯æ¸…å•

- [x] `llm_client.py` æ¥å— `history_messages` å‚æ•°
- [x] `llm_client.py` æ­£ç¡®æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆsystem + history + currentï¼‰
- [x] `llm_client.py` è¿”å› `debug_info`
- [x] `conversation_engine.py` ä»æ•°æ®åº“è·å–å†å²æ¶ˆæ¯ï¼ˆæœ€å¤š20æ¡ï¼‰
- [x] `conversation_engine.py` ä¼ é€’å†å²æ¶ˆæ¯ç»™ LLM
- [x] `conversation_engine.py` è°ƒè¯•æ¨¡å¼è§¦å‘è®°å¿†æ•´ç†
- [x] `chat.py` æ¥å— `debug_mode` å‚æ•°
- [x] `chat.py` è¿”å› `history_messages_count` å’Œ `debug_info`
- [x] `cli_chat.py` å®ç° `/debug on/off/status` å‘½ä»¤
- [x] `cli_chat.py` æ˜¾ç¤ºå®Œæ•´ Promptï¼ˆsystem + history + currentï¼‰

---

## âœ… æ€»ç»“

### å®ç°å®Œæˆ
1. âœ… ä¿®å¤å¯¹è¯å†å²ä¼ é€’é—®é¢˜
2. âœ… å®ç°è°ƒè¯•æ¨¡å¼å¯è§†åŒ–
3. âœ… è°ƒè¯•æ¨¡å¼ç«‹å³è§¦å‘è®°å¿†æ•´ç†
4. âœ… CLI è°ƒè¯•å‘½ä»¤
5. âœ… è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

### å¾…å……å€¼åéªŒè¯
1. â¸ï¸ å¤šè½®å¯¹è¯è®°å¿†æµ‹è¯•
2. â¸ï¸ NeuroMemory å¬å›éªŒè¯
3. â¸ï¸ è®°å¿†æ•´ç†è§¦å‘éªŒè¯

### æ€§èƒ½è€ƒè™‘
- **å†å²é™åˆ¶**: æœ€å¤š20è½®å¯¹è¯ï¼ˆ40æ¡æ¶ˆæ¯ï¼‰
- **Token æ¶ˆè€—**: çº¦ 500 tokens/è¯·æ±‚ï¼ˆvs ä¹‹å‰çš„ 100ï¼‰
- **å½±å“**: å¯æ¥å—ï¼Œä¿æŒå¯¹è¯è¿è´¯æ€§

---

**çŠ¶æ€**: å®ç°å·²å®Œæˆå¹¶é€šè¿‡ä»£ç å®¡æŸ¥ï¼Œç­‰å¾… API ä½™é¢å……å€¼åè¿›è¡Œå®é™…å¯¹è¯æµ‹è¯•ã€‚

**ä¸‹ä¸€æ­¥**: å……å€¼ DeepSeek API è´¦æˆ·åè¿è¡Œ `test_debug_mode.py` éªŒè¯å®Œæ•´åŠŸèƒ½ã€‚
